{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ed1133",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2699,
     "status": "ok",
     "timestamp": 1766149081448,
     "user": {
      "displayName": "Harshad Kumar",
      "userId": "04569833008376857969"
     },
     "user_tz": -330
    },
    "id": "60ed1133",
    "outputId": "e7a01cb9-d823-4a81-f968-05657e7b6042"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "drive.mount('/content/drive')\n",
    "ROOT_FOLDER = '/content/drive/My Drive/Project/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4mlAs02f-ghI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 17591,
     "status": "ok",
     "timestamp": 1766149099076,
     "user": {
      "displayName": "Harshad Kumar",
      "userId": "04569833008376857969"
     },
     "user_tz": -330
    },
    "id": "4mlAs02f-ghI",
    "outputId": "a032648f-eacb-42e1-ea84-a2f19b398627"
   },
   "outputs": [],
   "source": [
    "# Install ffmpeg\n",
    "!apt-get update\n",
    "!apt-get install -y ffmpeg\n",
    "\n",
    "# # Verify installation\n",
    "# !ffmpeg -version\n",
    "!pip install evaluate\n",
    "!pip install jiwer\n",
    "!pip install torchcodec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38PejWCmKO5I",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 5032,
     "status": "ok",
     "timestamp": 1766146403582,
     "user": {
      "displayName": "Harshad Kumar",
      "userId": "04569833008376857969"
     },
     "user_tz": -330
    },
    "id": "38PejWCmKO5I",
    "outputId": "be988adf-c3e1-463a-f3a0-28dcff2869b8"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qWaCbVbf-eTx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14064,
     "status": "ok",
     "timestamp": 1766149115852,
     "user": {
      "displayName": "Harshad Kumar",
      "userId": "04569833008376857969"
     },
     "user_tz": -330
    },
    "id": "qWaCbVbf-eTx",
    "outputId": "d004177a-1ff5-42d2-b584-54c202a435d7"
   },
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "from transformers import WhisperProcessor\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from datasets import Dataset, Audio\n",
    "from typing import Any, List, Dict, Union\n",
    "import torch\n",
    "import jiwer\n",
    "\n",
    "from transformers import (\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "\n",
    "import evaluate\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
    "processor = WhisperProcessor.from_pretrained(\n",
    "    \"openai/whisper-small\",\n",
    "    language=\"English\",      # sets English as decoding language\n",
    "    task=\"transcribe\",        # task can be 'transcribe' or 'translate'\n",
    "    feature_extractor_type=\"whisper\",  # Explicitly set feature extractor type\n",
    "    feature_size=80 # Set feature size to 80\n",
    ")\n",
    "model.generation_config.task = 'transcribe'\n",
    "model.generation_config.forced_decoder_ids = None\n",
    "# from peft import LoraConfig, PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "P2udBw6FEC5v",
   "metadata": {
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1766149118738,
     "user": {
      "displayName": "Harshad Kumar",
      "userId": "04569833008376857969"
     },
     "user_tz": -330
    },
    "id": "P2udBw6FEC5v"
   },
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "data_set_path = os.path.join(ROOT_FOLDER, \"data\", \"data_sets\",'prepared_val')\n",
    "val_dataset = load_from_disk(data_set_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2LzQ2s9E19E",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 519,
     "status": "ok",
     "timestamp": 1766149120049,
     "user": {
      "displayName": "Harshad Kumar",
      "userId": "04569833008376857969"
     },
     "user_tz": -330
    },
    "id": "b2LzQ2s9E19E",
    "outputId": "58bfa34d-441c-4616-82e7-b7986005e5a6"
   },
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, concatenate_datasets\n",
    "base_dir = os.path.join(ROOT_FOLDER, \"data\", \"data_sets\")\n",
    "\n",
    "parts = []\n",
    "n_parts = 20\n",
    "\n",
    "for i in range(1, n_parts + 1):\n",
    "    data_set_path = os.path.join(base_dir, f\"prepared_train_part{i}\")\n",
    "    if not os.path.isdir(data_set_path):\n",
    "        print(f\"Warning: missing directory {data_set_path}\")\n",
    "        continue\n",
    "    ds = load_from_disk(data_set_path)\n",
    "    parts.append(ds)\n",
    "    print(f\"Loaded part {i}: {data_set_path} with {len(ds)} rows\")\n",
    "\n",
    "# Optionally concatenate into a single dataset\n",
    "if parts:\n",
    "    train_dataset = concatenate_datasets(parts)\n",
    "    print(f\"Concatenated dataset rows: {len(train_dataset)}\")\n",
    "else:\n",
    "    train_dataset = None\n",
    "    print(\"No parts loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebc66d2",
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1766149120897,
     "user": {
      "displayName": "Harshad Kumar",
      "userId": "04569833008376857969"
     },
     "user_tz": -330
    },
    "id": "bebc66d2"
   },
   "outputs": [],
   "source": [
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "\n",
    "    def __init__(self, processor: Any, decoder_start_token_id: int):\n",
    "        self.processor = processor\n",
    "        self.decoder_start_token_id = decoder_start_token_id\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # Use processor.feature_extractor to extract features\n",
    "        input_features = [{'input_features': self.processor.feature_extractor(feature['path']['array'], sampling_rate=feature['path']['sampling_rate']).input_features[0]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors='pt')\n",
    "\n",
    "        label_features = [{'input_ids': feature['labels'][0:448]} for feature in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors='pt')\n",
    "\n",
    "        labels = labels_batch['input_ids'].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch['labels'] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
    ")\n",
    "\n",
    "# metric = evaluate.load('wer')\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Concatenate for MICRO / WEIGHTED WER\n",
    "    concatenated_pred = \" \".join(pred_str)\n",
    "    concatenated_ref = \" \".join(label_str)\n",
    "\n",
    "    weighted_wer = 100 * jiwer.wer(\n",
    "        concatenated_ref,\n",
    "        concatenated_pred\n",
    "    )\n",
    "\n",
    "    return {'wer': weighted_wer}\n",
    "\n",
    "# def compute_metrics(pred):\n",
    "#     pred_ids = pred.predictions\n",
    "#     label_ids = pred.label_ids\n",
    "\n",
    "#     # Replace -100 with pad token\n",
    "#     label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "#     # Decode\n",
    "#     pred_str = processor.tokenizer.batch_decode(\n",
    "#         pred_ids, skip_special_tokens=True\n",
    "#     )\n",
    "#     label_str = processor.tokenizer.batch_decode(\n",
    "#         label_ids, skip_special_tokens=True\n",
    "#     )\n",
    "\n",
    "#     total_edits = 0\n",
    "#     total_ref_words = 0\n",
    "\n",
    "#     for hyp, ref in zip(pred_str, label_str):\n",
    "#         hyp_words = hyp.strip().split()\n",
    "#         ref_words = ref.strip().split()\n",
    "\n",
    "#         # Skip empty references to avoid divide-by-zero\n",
    "#         if len(ref_words) == 0:\n",
    "#             continue\n",
    "\n",
    "#         # Compute word-level edit distance\n",
    "#         measures = jiwer.compute_measures(ref, hyp)\n",
    "\n",
    "#         total_edits += measures[\"substitutions\"] \\\n",
    "#                      + measures[\"deletions\"] \\\n",
    "#                      + measures[\"insertions\"]\n",
    "\n",
    "#         total_ref_words += len(ref_words)\n",
    "\n",
    "#     weighted_wer = 100.0 * total_edits / total_ref_words\n",
    "\n",
    "#     return {\n",
    "#         \"wer\": weighted_wer\n",
    "#     }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39623c5",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1766149121511,
     "user": {
      "displayName": "Harshad Kumar",
      "userId": "04569833008376857969"
     },
     "user_tz": -330
    },
    "id": "a39623c5"
   },
   "outputs": [],
   "source": [
    "#parameters\n",
    "out_dir = os.path.join(ROOT_FOLDER, 'finetuned_model_files', 'Small_full_training')\n",
    "batch_size = 16\n",
    "gradient_accum = 1\n",
    "epochs = 45\n",
    "dataloader_num = 16\n",
    "\n",
    "\n",
    "#save/eval strategy at every 3 epoch\n",
    "step_size = int(train_dataset.shape[0]/(batch_size*gradient_accum) + 1)*3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca42705b",
   "metadata": {
    "executionInfo": {
     "elapsed": 131,
     "status": "ok",
     "timestamp": 1766149122716,
     "user": {
      "displayName": "Harshad Kumar",
      "userId": "04569833008376857969"
     },
     "user_tz": -330
    },
    "id": "ca42705b"
   },
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=out_dir,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    gradient_accumulation_steps= gradient_accum,\n",
    "    learning_rate=1*10**-6,\n",
    "    # warmup_steps=300,\n",
    "\n",
    "    #add regularization\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "\n",
    "    bf16=False,\n",
    "    fp16=True,\n",
    "    num_train_epochs=epochs,\n",
    "    logging_strategy='epoch',\n",
    "    #save_strategy='epoch',\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=448,\n",
    "    report_to=['tensorboard'],\n",
    "    load_best_model_at_end=False,\n",
    "    metric_for_best_model='wer',\n",
    "    greater_is_better=False,\n",
    "    dataloader_num_workers= dataloader_num,\n",
    "    save_total_limit=4,\n",
    "    seed=42,\n",
    "    data_seed=42,\n",
    "    remove_unused_columns=False, # Add this line\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=step_size,\n",
    "    save_strategy='steps',\n",
    "    save_steps=step_size\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UJ4O5XTP4WSw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1766149124512,
     "user": {
      "displayName": "Harshad Kumar",
      "userId": "04569833008376857969"
     },
     "user_tz": -330
    },
    "id": "UJ4O5XTP4WSw",
    "outputId": "2f9b8f2f-7eda-40d0-a23a-e96d4db6fc3b"
   },
   "outputs": [],
   "source": [
    "# from peft import LoraConfig, get_peft_model\n",
    "# lora_config = LoraConfig(\n",
    "#     r=64,             # Rank\n",
    "#     lora_alpha=128,   # Scaling\n",
    "#     target_modules= [\"q_proj\",\"k_proj\",\"v_proj\",\"out_proj\"],  # Attention projection modules (important for Whisper)\n",
    "#     lora_dropout=0.1,\n",
    "#     bias=\"none\",\n",
    "#     # task_type=\"SEQ_2_SEQ_LM\"\n",
    "# )\n",
    "\n",
    "# model = get_peft_model(model, lora_config)\n",
    "# model.print_trainable_parameters()   # Debug: shows % of params tuned\n",
    "\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable: {trainable_params}\")\n",
    "print(f\"Total: {total_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bvnDiTezRdrV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 358,
     "status": "ok",
     "timestamp": 1766149126052,
     "user": {
      "displayName": "Harshad Kumar",
      "userId": "04569833008376857969"
     },
     "user_tz": -330
    },
    "id": "bvnDiTezRdrV",
    "outputId": "69f01651-d210-4d0c-e6a2-a9af0eb1a7be"
   },
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0232c85a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 4006151,
     "status": "error",
     "timestamp": 1766153132940,
     "user": {
      "displayName": "Harshad Kumar",
      "userId": "04569833008376857969"
     },
     "user_tz": -330
    },
    "id": "0232c85a",
    "outputId": "8e9af4f6-0fea-415c-a7bb-97af5046a49d"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_nn3jO8WbEL-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1474695,
     "status": "ok",
     "timestamp": 1766046932828,
     "user": {
      "displayName": "Harshad Kumar",
      "userId": "04569833008376857969"
     },
     "user_tz": -330
    },
    "id": "_nn3jO8WbEL-",
    "outputId": "e3f419d4-3327-4f69-e8ae-67dfc2be39b1"
   },
   "outputs": [],
   "source": [
    "trainer.train(\n",
    "    resume_from_checkpoint=os.path.join(out_dir, \"checkpoint-4650\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e5bf0a",
   "metadata": {
    "id": "b5e5bf0a"
   },
   "outputs": [],
   "source": [
    "trainer.save_model(os.path.join(out_dir, \"best_model\"))\n",
    "\n",
    "#to get latest checkpoint use , go to latest check_point\n",
    "# To find the actual best checkpoint, you need to look at the trainer_state.json file generated in your output directory (or inside any checkpoint-XXX folder).\n",
    "# Open output_dir/checkpoint-XXX/trainer_state.json.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879c7524",
   "metadata": {
    "id": "879c7524"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dWW35ZnRJiES",
   "metadata": {
    "id": "dWW35ZnRJiES"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
